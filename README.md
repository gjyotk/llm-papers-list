# llm-papers

## Overview

Every week, I set out to explore, reflect on, and document my learnings from one LLM-based research paper. This journey is a personal effort to stay connected with the latest developments, deepen my understanding of Machine Learning, and share my insights with others who share this passion. I'll use this repository as a space to chart my progress, with summaries, challenges addressed by the papers, and links to any related blogs or discussions that resonate with me.

## Papers List

Below is the list of most relevant foundational papers on LLMs (according to me) that I've curated from Latent Space's 2025 Papers list (https://www.latent.space/p/2025-papers#%C2%A7section-frontier-llms) and suggested reading materials from the course "Generative AI with Large Language Models" by Deeplearning.ai on Coursera:


1. **[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)**

   *Alec Radford et al., 2018*

2. **[Attention is All You Need](https://arxiv.org/abs/1706.03762)**

   *Ashish Vaswani et al., 2017*

## Weekly Progress

Summary:

Challenges the Paper Solves:

Key Contributions:

Personal Insights/Takeaways:

Related Blog or Discussions:


## How to Use This Repository

You can follow my progress by checking the updates in this README or visiting this URL: . Feel free to share your thoughts, suggest additional papers, or discuss the content via GitHub issues or pull requests.

Stay tuned for weekly updates!


